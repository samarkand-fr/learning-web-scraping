# üìö Books to Scrape  - Syst√®me de Surveillance des Prix

Ce projet est un outil de web scraping d√©velopp√© en Python pour extraire des donn√©es du site [Books to Scrape](http://books.toscrape.com/). Il a √©t√© con√ßu de mani√®re modulaire pour couvrir diff√©rents besoins, de l'extraction d'un livre unique √† la r√©cup√©ration compl√®te du catalogue avec images.

## Architecture du Projet

Le projet est modulaire, avec une orchestration centrale assur√©e par main.py.
Les responsabilit√©s sont s√©par√©es par phase, et les fonctions communes sont mutualis√©es dans un module utilitaire afin de respecter le principe DRY "Don't Repeat Yourself".

- **`main.py`** : Le point d'entr√©e principal. Il propose un menu interactif pour lancer les diff√©rentes phases.
- **`scraper_utils.py`** : Contient toute la logique technique (extraction, pagination, sauvegarde CSV, t√©l√©chargement d'images) et les constantes globales.
- **`phase1.py` √† `phase4.py`** :  Scripts ind√©pendants contenant la logique propre √† chaque phase et qui utilisent les fonctions de `scraper_utils.py`.

## Installation

1. **Clonez le repository** :
   ```bash
   git clone https://github.com/samarkand-fr/learning-web-scraping
   cd ./learning-web-scraping/
   ```

2. **Cr√©ez un environnement virtuel** :
   ```bash
   python -m venv venv
   ```

3. **Activez l'environnement virtuel** :
   - **Sur Mac/Linux** :
     ```bash
     source venv/bin/activate
     ```
   - **Sur Windows** :
     ```bash
     venv\Scripts\activate
     ```

4. **Installez les d√©pendances** :
   ```bash
   pip install -r requirements.txt
   ```

## Utilisation (Recommand√©)

Lancez simplement le script principal pour acc√©der au menu interactif :

```bash
python main.py
```

Le menu vous proposera 4 options :
1. **Phase 1** : Extraire les donn√©es d'un livre sp√©cifique (ex: 'wuthering heights').
2. **Phase 2** : Extraire tous les livres d'une cat√©gorie choisie (ex: 'food and drink').
3. **Phase 3** : Extraire l'int√©gralit√© du catalogue (toutes les livres) dans des fichiers CSV par cat√©gorie.
4. **Phase 4** : Extraire et t√©l√©charger l'ensemble des images du site.

## R√©sultats

Toutes les donn√©es g√©n√©r√©es sont centralis√©es dans le dossier **`scraped_data/`** (ignor√© par Git) :

- **Fichiers CSV** : Sauvegard√©s dans `scraped_data/csv/` (un fichier par cat√©gorie).
- **Images** : Enregistr√©es dans `scraped_data/images/`, class√©es par sous-dossiers de cat√©gorie.

## D√©pendances
- `requests` : Pour effectuer les requ√™tes HTTP.
- `beautifulsoup4` : Pour analyser le code HTML des pages.
